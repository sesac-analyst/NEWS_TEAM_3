{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 카테고리 코드와 이름 매핑\n",
    "category_mapping = {\n",
    "    '259': '금융',\n",
    "    '258': '증권',\n",
    "    '261': '산업/제계',\n",
    "    '771': '중기/벤처',\n",
    "    '260': '부동산',\n",
    "    '262': '글로벌 경제',\n",
    "    '310': '생활경제',\n",
    "    '263': '경제일반'\n",
    "}\n",
    "\n",
    "# URL에서 카테고리 코드를 추출하고, 카테고리 이름 반환\n",
    "def get_category_from_url(url):\n",
    "    category_id = url.split('/')[-1].split('?')[0]  # URL의 마지막 부분에서 카테고리 번호 추출\n",
    "    return category_mapping.get(category_id, '기타')  # 매핑되지 않은 경우 '기타' 반환\n",
    "\n",
    "# 페이지 끝까지 스크롤하여 콘텐츠 로드\n",
    "def scroll_to_bottom(driver, pause_time=1):\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(pause_time)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        if new_height == last_height:\n",
    "            try:\n",
    "                more_button = WebDriverWait(driver, 3).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \"div > div.section_more > a\"))\n",
    "                )\n",
    "                more_button.click()\n",
    "                time.sleep(pause_time)\n",
    "            except Exception as e:\n",
    "                print(f\"더보기 버튼 클릭 오류: {e}\")\n",
    "                break\n",
    "        \n",
    "        last_height = new_height\n",
    "\n",
    "# 기사 세부 정보를 추출하는 함수\n",
    "def extract_article_details(url):\n",
    "    try:\n",
    "        print(f\"기사 정보 추출 중: {url}\")\n",
    "        res = requests.get(url)\n",
    "        soup_in = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        title = soup_in.find('h2').text if soup_in.find('h2') else '제목 오류'\n",
    "\n",
    "        author = '기자명 오류'\n",
    "        if soup_in.find('em', {\"class\": \"media_end_head_journalist_name\"}):\n",
    "            author = soup_in.find('em', {\"class\": \"media_end_head_journalist_name\"}).text\n",
    "        elif soup_in.find('span', {\"class\": \"byline_s\"}):\n",
    "            author = soup_in.find('span', {\"class\": \"byline_s\"}).text[:3] + \" 기자\"\n",
    "\n",
    "        content = soup_in.find('article', {\"id\": \"dic_area\"}).text if soup_in.find('article', {\"id\": \"dic_area\"}) else '본문 오류'\n",
    "        date = soup_in.find('span', {\"class\": \"media_end_head_info_datestamp_time _ARTICLE_DATE_TIME\"}).text if soup_in.find('span', {\"class\": \"media_end_head_info_datestamp_time _ARTICLE_DATE_TIME\"}) else '날짜 오류'\n",
    "        mod_date = soup_in.find('span', {\"class\": \"media_end_head_info_datestamp_time _ARTICLE_MODIFY_DATE_TIME\"}).text if soup_in.find('span', {\"class\": \"media_end_head_info_datestamp_time _ARTICLE_MODIFY_DATE_TIME\"}) else '수정 날짜 없음'\n",
    "\n",
    "        return title, author, content, date, mod_date\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"기사 세부 정보 추출 중 오류 발생: {e}\")\n",
    "        return '오류', '오류', '오류', '오류', '오류'\n",
    "\n",
    "# 댓글 데이터를 추출하는 함수\n",
    "def extract_comments(article_url):\n",
    "    news_id = article_url.split(\"/\")[-1]\n",
    "    press_id = article_url.split(\"/\")[-2]\n",
    "    news_object = f\"news{press_id}%2C{news_id}\"\n",
    "\n",
    "    url = f\"https://apis.naver.com/commentBox/cbox/web_naver_list_jsonp.json?ticket=news&templateId=view_economy_m1&pool=cbox5&_cv=20240826125754&lang=ko&country=KR&objectId={news_object}&categoryId=&pageSize=100&indexSize=10&groupId=&listType=OBJECT&pageType=more&page=1&initialize=true&followSize=5&userType=&useAltSort=true&replyPageSize=20&sort=FAVORITE&includeAllStatus=true&_=1725273489605\"\n",
    "\n",
    "    custom_header = {\n",
    "        \"Referer\": article_url,\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=custom_header)\n",
    "\n",
    "    json_string = response.text.split(\"_callback(\")[-1].strip(');')\n",
    "\n",
    "    try:\n",
    "        json_data = json.loads(json_string)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON 파싱 오류: {e}\")\n",
    "        return []\n",
    "\n",
    "    comments = []\n",
    "    if 'result' in json_data and 'commentList' in json_data['result']:\n",
    "        for comment in json_data['result']['commentList']:\n",
    "            user_id_no = comment.get(\"userIdNo\", \"unknown\")\n",
    "            user_unique = user_id_no[:5] if user_id_no else \"unknown\"\n",
    "\n",
    "            comments.append({\n",
    "                \"user_unique\": user_unique,  # user_id 대신 user_unique 사용\n",
    "                \"comment\": comment.get(\"contents\"),\n",
    "                \"comment_date\": comment.get(\"regTime\"),\n",
    "                \"update_date\": comment.get(\"modTime\", \"수정되지 않음\")\n",
    "            })\n",
    "    return comments\n",
    "\n",
    "# 뉴스 기사와 댓글을 크롤링하는 함수\n",
    "def scrape_news(date_range, base_url):\n",
    "    title_list, content_list, author_list = [], [], []\n",
    "    date_list, update_date_list, publisher_list = [], [], []\n",
    "    url_list, comments_list, category_list = [], [], []\n",
    "\n",
    "    category = get_category_from_url(base_url)\n",
    "\n",
    "    for date in date_range:\n",
    "        url = f\"{base_url}?date={date.strftime('%Y%m%d')}\"\n",
    "        print(f\"크롤링 중: {url}\")\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "\n",
    "        scroll_to_bottom(driver)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        links = soup.select(\"div.sa_text > a\")\n",
    "        publishers = soup.select(\"div.sa_text_press\")\n",
    "\n",
    "        for link, publisher in zip(links, publishers):\n",
    "            article_url = link.get('href')\n",
    "            publisher_name = publisher.text.strip()\n",
    "            url_list.append(article_url)\n",
    "            publisher_list.append(publisher_name)\n",
    "            category_list.append(category)\n",
    "\n",
    "            title, author, content, date, mod_date = extract_article_details(article_url)\n",
    "            title_list.append(title)\n",
    "            author_list.append(author)\n",
    "            content_list.append(content)\n",
    "            date_list.append(date)\n",
    "            update_date_list.append(mod_date)\n",
    "\n",
    "            comments = extract_comments(article_url)\n",
    "            comments_list.extend(comments)\n",
    "\n",
    "            print(f\"기사 URL: {article_url}, 발행자: {publisher_name}, 카테고리: {category}\")\n",
    "\n",
    "    return {\n",
    "        \"title\": title_list,\n",
    "        \"author\": author_list,\n",
    "        \"content\": content_list,\n",
    "        \"publication_date\": date_list,\n",
    "        \"update_date\": update_date_list,\n",
    "        \"article_url\": url_list,\n",
    "        \"publisher\": publisher_list,\n",
    "        \"category\": category_list,\n",
    "        \"comments\": comments_list\n",
    "    }\n",
    "\n",
    "# 날짜 범위를 생성하는 함수\n",
    "def generate_date_range(start_date, end_date):\n",
    "    start = datetime.strptime(start_date, '%Y%m%d')\n",
    "    end = datetime.strptime(end_date, '%Y%m%d')\n",
    "    delta = timedelta(days=1)\n",
    "    while start <= end:\n",
    "        yield start\n",
    "        start += delta\n",
    "\n",
    "# 수집한 데이터를 CSV 파일로 저장하는 함수\n",
    "def save_to_csv(start_date, end_date, all_scraped_data):\n",
    "    \"\"\"스크래핑한 데이터를 CSV 파일로 저장하는 함수\"\"\"\n",
    "    # 파일 이름 생성\n",
    "    file_name_articles = f\"{start_date}-{end_date}_articles.csv\"\n",
    "    file_name_comments = f\"{start_date}-{end_date}_comments.csv\"\n",
    "\n",
    "    # 기사 데이터 저장\n",
    "    df_articles = pd.DataFrame({\n",
    "        'platform': '네이버',\n",
    "        'category': all_scraped_data['category'],  # 카테고리 자동 설정\n",
    "        'publisher': all_scraped_data['publisher'],\n",
    "        'publication_date': all_scraped_data['publication_date'],\n",
    "        'title': all_scraped_data['title'],\n",
    "        'content': all_scraped_data['content'],\n",
    "        'author': all_scraped_data['author'],  # 기자 이름 컬럼 'author'로 수정\n",
    "        'article_url': all_scraped_data['article_url'],\n",
    "        'update_date': all_scraped_data['update_date']\n",
    "    })\n",
    "\n",
    "    # 댓글 데이터 저장\n",
    "    df_comments = pd.DataFrame({\n",
    "        'user_unique': [comment['user_unique'] for comment in all_scraped_data['comments']],\n",
    "        'comment': [comment['comment'] for comment in all_scraped_data['comments']],\n",
    "        'comment_date': [comment['comment_date'] for comment in all_scraped_data['comments']],\n",
    "        'update_date': [comment['update_date'] for comment in all_scraped_data['comments']]\n",
    "    })\n",
    "\n",
    "    # 데이터프레임을 CSV로 저장\n",
    "    df_articles.to_csv(file_name_articles, index=False)\n",
    "    df_comments.to_csv(file_name_comments, index=False)\n",
    "\n",
    "    print(f\"기사 데이터가 '{file_name_articles}' 파일로 저장되었습니다.\")\n",
    "    print(f\"댓글 데이터가 '{file_name_comments}' 파일로 저장되었습니다.\")\n",
    "\n",
    "# 크롬 드라이버 설정 및 크롤링 실행\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "start_date = '20240801'\n",
    "end_date = '20240831'\n",
    "date_range = generate_date_range(start_date, end_date)\n",
    "\n",
    "# 카테고리별 URL\n",
    "base_urls = [\n",
    "    \"https://news.naver.com/breakingnews/section/101/258\",  # 증권\n",
    "#     \"https://news.naver.com/breakingnews/section/101/259\",  # 금융\n",
    "#     \"https://news.naver.com/breakingnews/section/101/261\",  # 산업/제계\n",
    "#     \"https://news.naver.com/breakingnews/section/101/771\",  # 중기/벤처\n",
    "#     \"https://news.naver.com/breakingnews/section/101/260\",  # 부동산\n",
    "#     \"https://news.naver.com/breakingnews/section/101/262\",  # 글로벌 경제\n",
    "#     \"https://news.naver.com/breakingnews/section/101/310\",  # 생활경제\n",
    "#     \"https://news.naver.com/breakingnews/section/101/263\"   # 경제일반\n",
    "]\n",
    "\n",
    "all_scraped_data = {\n",
    "    \"title\": [],\n",
    "    \"author\": [],\n",
    "    \"content\": [],\n",
    "    \"publication_date\": [],\n",
    "    \"update_date\": [],\n",
    "    \"article_url\": [],\n",
    "    \"publisher\": [],\n",
    "    \"category\": [],\n",
    "    \"comments\": []\n",
    "}\n",
    "\n",
    "# 각 카테고리별로 데이터를 크롤링하고 수집\n",
    "for base_url in base_urls:\n",
    "    scraped_data = scrape_news(date_range, base_url)\n",
    "\n",
    "    # 수집한 데이터를 합치기\n",
    "    for key in all_scraped_data:\n",
    "        all_scraped_data[key].extend(scraped_data[key])\n",
    "\n",
    "# 크롬 드라이버 종료\n",
    "driver.quit()\n",
    "\n",
    "# 수집된 데이터를 CSV로 저장\n",
    "save_to_csv(start_date, end_date, all_scraped_data)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
