{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01250b9f-b39b-4fca-a4e8-da673e7b38b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Articles:   1%|          | 2/263 [01:06<2:25:09, 33.37s/it]"
     ]
    }
   ],
   "source": [
    "#csv 파일 저장(컬럼명은 DB에서 나온대로 저장)\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "base_url = \"https://news.naver.com/breakingnews/section/101/262?date=\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\", \"Referer\": \"https://www.nate.com/\"\n",
    "}\n",
    "\n",
    "output_file = r\"C:\\Users\\cobe2\\Documents\\SeSAC\\PJ\\nnews_Global.csv\"\n",
    "\n",
    "def extract_and_save_articles(soup, date, writer):\n",
    "    li_tg = soup.select('ul.sa_list > li > div.sa_item_inner > div.sa_item_flex > div.sa_thumb > div.sa_thumb_inner > a')\n",
    "    for i in li_tg:\n",
    "        crawling_url = i.attrs['href']\n",
    "        response = requests.get(crawling_url, headers=headers)\n",
    "        crawling_soup = BeautifulSoup(response.text, 'html.parser')  \n",
    "        title_tag = crawling_soup.select_one('h2.media_end_head_headline')\n",
    "        title = title_tag.text if title_tag else \"No title Found\"       \n",
    "        date_li_tag = crawling_soup.select_one('span.media_end_head_info_datestamp_time._ARTICLE_DATE_TIME')\n",
    "        formatted_date = date_li_tag.text.replace(\"오후\", \"\").replace('.', '_').replace(':', '_').replace(' ', '').replace('오전', '') if date_li_tag else \"No Date Found\"\n",
    "        date_mo_tag = crawling_soup.select_one('span.media_end_head_info_datestamp_time._ARTICLE_MODIFY_DATE_TIME')\n",
    "        modify_date = date_mo_tag.text.replace(\"오후\",\"\").replace('.', '_').replace(':', '_').replace(' ','').replace('오전', '') if date_mo_tag else \" \"\n",
    "        source_img_tag = crawling_soup.select_one('div.media_end_head_top a img')\n",
    "        source = source_img_tag['alt'] if source_img_tag else \"No Source Found\"\n",
    "\n",
    "        Nnews_tag = crawling_soup.select_one('#dic_area')\n",
    "        Nnews = Nnews_tag.text.replace('\\n', '').replace('\\r', '').replace('\\t', '') if Nnews_tag else \"No Content Found\"\n",
    "        Writer_tag = crawling_soup.select_one('span.byline_s')\n",
    "        Writer = Writer_tag.text if Writer_tag else \"No Writer Found\"\n",
    "\n",
    "        writer.writerow([formatted_date, modify_date, title, Nnews, source, crawling_url, Writer])\n",
    "\n",
    "def get_articles_for_date(date, writer):\n",
    "    current_url = f\"{base_url}{date.strftime('%Y%m%d')}\"\n",
    "    response = requests.get(current_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    iteration = 0\n",
    "    extract_and_save_articles(soup, date, writer)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        next_meta_div = soup.select_one('div.section_latest_article_CONTENT_LIST_PERSIST_META')\n",
    "        \n",
    "        if next_meta_div:\n",
    "            next_cursor = next_meta_div.get('data-cursor', None)\n",
    "        else:\n",
    "            #print(f\"No more articles to load for {date.strftime('%Y-%m-%d')}.\")\n",
    "            break  \n",
    "\n",
    "        if next_cursor:\n",
    "\n",
    "            next_url = f\"{current_url}&data-cursor={next_cursor}\"\n",
    "\n",
    "            response = requests.get(next_url, headers=headers)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            iteration += 1\n",
    "            extract_and_save_articles(soup, date, writer)\n",
    "\n",
    "            time.sleep(1)\n",
    "        else:\n",
    "            print(f\"No more data-cursor found for {date.strftime('%Y-%m-%d')}.\")\n",
    "            break  \n",
    "\n",
    "start_date = datetime.strptime(\"20231212\", \"%Y%m%d\")\n",
    "end_date = datetime.strptime(\"20240830\", \"%Y%m%d\")\n",
    "\n",
    "current_date = start_date\n",
    "\n",
    "file_exists = os.path.isfile(output_file)\n",
    "\n",
    "with open(output_file, 'a' if file_exists else 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    if not file_exists:\n",
    "        writer.writerow(['publication_date', 'update_date', 'title', 'content', 'publisher_id', 'article_url', 'author'])\n",
    "    \n",
    "    date_range = (end_date - start_date).days + 1\n",
    "    for current_date in tqdm([start_date + timedelta(days=i) for i in range(date_range)], desc=\"Crawling Articles\"):\n",
    "        get_articles_for_date(current_date, writer)\n",
    "\n",
    "print(\"Crawling End\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fdd473-c11a-4f45-8537-ffe15a1df899",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
