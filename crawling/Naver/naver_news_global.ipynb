{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f02b0a3-3ee9-4cb2-8d0c-5cb9edb16b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more articles to load for 2023-08-30.\n",
      "Crawling End\n"
     ]
    }
   ],
   "source": [
    "#네이버뉴스 글로벌 경제(3시 55분 시작)\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "base_url = \"https://news.naver.com/breakingnews/section/101/262?date=\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# 기사 저장 디렉토리\n",
    "output_dir = r\"C:\\Users\\cobe2\\Documents\\SeSAC\\PJ\\nnews_Global\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 기사 본문 크롤링 함수(1번)\n",
    "def extract_and_save_articles(soup, date, iteration):\n",
    "    li_tg = soup.select('ul.sa_list > li > div.sa_item_inner > div.sa_item_flex > div.sa_thumb > div.sa_thumb_inner > a')\n",
    "    for i in li_tg:\n",
    "        crawling_url = i.attrs['href']\n",
    "        response = requests.get(crawling_url, headers=headers)\n",
    "        crawling_soup = BeautifulSoup(response.text, 'html.parser')  # 해당 뉴스기사 링크의 html 정보 추출\n",
    "\n",
    "        # 뉴스기사 개별 사이트로 들어가서 제목, 날짜(수정 날짜 포함), 언론사 있는 경우만 추출\n",
    "        title_tag = crawling_soup.select_one('h2.media_end_head_headline')\n",
    "        title = title_tag.text if title_tag else \"No title Found\"       \n",
    "        date_li_tag = crawling_soup.select_one('span.media_end_head_info_datestamp_time._ARTICLE_DATE_TIME')\n",
    "        formatted_date = date_li_tag.text.replace(\"오후\", \"\").replace('.', '_').replace(':', '_').replace(' ', '').replace('오전', '') if date_li_tag else \"No Date Found\"\n",
    "        date_mo_tag = crawling_soup.select_one('span.media_end_head_info_datestamp_time._ARTICLE_MODIFY_DATE_TIME')\n",
    "        modify_date = date_mo_tag.text.replace(\"오후\",\"\").replace('.', '_').replace(':', '_').replace(' ','').replace('오전', '') if date_mo_tag else \" \"\n",
    "        source_img_tag = crawling_soup.select_one('div.media_end_head_top a img')\n",
    "        source = source_img_tag['alt'] if source_img_tag else \"No Source Found\"\n",
    "\n",
    "        # 기사본문, 기자이름(이메일포함)이 있는 경우만 추출\n",
    "        Nnews_tag = crawling_soup.select_one('#dic_area')\n",
    "        Nnews = Nnews_tag.text.replace('\\n', '').replace('\\r', '').replace('\\t', '') if Nnews_tag else \"No Content Found\"\n",
    "        Writer_tag = crawling_soup.select_one('span.byline_s')\n",
    "        Writer = Writer_tag.text if Writer_tag else \"No Writer Found\"\n",
    "        # 날짜가 들어가 있는 파일이름을 text 파일로 저장\n",
    "        file_path = os.path.join(output_dir, f\"{formatted_date}_네이버뉴스(글로벌).txt\")\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(f'제목\\n{title}\\n입력\\n{formatted_date}\\n수정\\n{modify_date}\\nSource\\n{source}\\n내용\\n{Nnews}\\n링크\\n{crawling_url}\\n\\n기자\\n{Writer}')\n",
    "        #print(f'Saved file: {file_path}')\n",
    "\n",
    "# 특정 날짜에 있는 기사를 크롤링(2번)\n",
    "def get_articles_for_date(date):\n",
    "    current_url = f\"{base_url}{date.strftime('%Y%m%d')}\"\n",
    "    response = requests.get(current_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    iteration = 0\n",
    "    extract_and_save_articles(soup, date, iteration)\n",
    "\n",
    "    while True:\n",
    "        # \"data-cursor\"=기사 더보기 찾아서 다음 기사를 요청\n",
    "        next_meta_div = soup.select_one('div.section_latest_article_CONTENT_LIST_PERSIST_META')\n",
    "        \n",
    "        if next_meta_div:\n",
    "            next_cursor = next_meta_div.get('data-cursor', None)\n",
    "        else:\n",
    "            print(f\"No more articles to load for {date.strftime('%Y-%m-%d')}.\")\n",
    "            break  # 커서가 없거나 커서를 눌렀는데 기사가 없을 경우 다음 날짜로 넘어감\n",
    "\n",
    "        if next_cursor:\n",
    "            # 다음 기사 요청을 위한 url 만들기\n",
    "            next_url = f\"{current_url}&data-cursor={next_cursor}\"\n",
    "\n",
    "            # request 보내기\n",
    "            response = requests.get(next_url, headers=headers)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # 기사 더보기로 생긴 새로운 기사들을 크롤링 후 저장하는 함수(1번)로 돌아간다 \n",
    "            iteration += 1\n",
    "            extract_and_save_articles(soup, date, iteration)\n",
    "\n",
    "            time.sleep(1)\n",
    "        else:\n",
    "            print(f\"No more data-cursor found for {date.strftime('%Y-%m-%d')}.\")\n",
    "            break  # data-cursor가 없으면 break\n",
    "\n",
    "# 날짜 정하기\n",
    "start_date = datetime.strptime(\"20230830\", \"%Y%m%d\")\n",
    "end_date = datetime.strptime(\"20230830\", \"%Y%m%d\")\n",
    "\n",
    "# 날짜별로 2번 함수를 작동시킴 \n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    \n",
    "    #print(f\"Crawling articles for {current_date.strftime('%Y-%m-%d')}\")\n",
    "    get_articles_for_date(current_date)\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "print(\"Crawling End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ddb10c-5472-45e2-a88b-7a7e24c9a140",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
