{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing_1: 100%|██████████| 367/367 [3:10:42<00:00, 31.18s/it]   \n",
      "Processing_2:  37%|███▋      | 130592/351323 [8:26:37<6:29:50,  9.44it/s]     "
     ]
    }
   ],
   "source": [
    "# 제목 전부 (썽공)\n",
    "import requests as rq\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "all_title = []\n",
    "previous_titles = None\n",
    "date_range = pd.date_range(start=\"2023-08-31\", end=\"2024-08-31\").strftime('%Y%m%d')\n",
    "\n",
    "for date in date_range:\n",
    "    page = 1\n",
    "    while True:\n",
    "        res = rq.get(f'https://news.daum.net/breakingnews/society/education?page={page}&regDate={date}', headers={\n",
    "            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36'\n",
    "        })\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        titles = soup.select('#mArticle > div.box_etc > ul > li:nth-child(-n+15) > div > strong > a')\n",
    "        next_button = soup.select_one('#mArticle > div.box_etc > div > span > a.btn_page.btn_next > span')\n",
    "        current_titles = [title.text for title in titles]\n",
    "\n",
    "        if not current_titles and not next_button:\n",
    "            break\n",
    "\n",
    "        if previous_titles == current_titles:\n",
    "            break\n",
    "\n",
    "        all_title.extend(current_titles)\n",
    "        previous_titles = current_titles\n",
    "\n",
    "        page += 1\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 언론사 및 링크 (성공)\n",
    "import requests as rq\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "all_Name = []\n",
    "all_links = []\n",
    "previous_titles = None\n",
    "date_range = pd.date_range(start=\"2023-08-31\", end=\"2024-08-31\").strftime('%Y%m%d')\n",
    "\n",
    "for date in date_range:\n",
    "    page = 1\n",
    "    while True:\n",
    "        res = rq.get(f'https://news.daum.net/breakingnews/society/affair?page={page}&regDate={date}', headers={\n",
    "            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36'\n",
    "        })\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        titles = soup.select('#mArticle > div.box_etc > ul > li:nth-child(-n+15) > div > strong > a')\n",
    "        Names = soup.select('span.info_news')\n",
    "        next_button = soup.select_one('#mArticle > div.box_etc > div > span > a.btn_page.btn_next > span')\n",
    "        current_titles = [title.text for title in titles]\n",
    "        links = soup.select('#mArticle .box_etc strong > a')\n",
    "\n",
    "        if not current_titles and not next_button:\n",
    "            break\n",
    "\n",
    "        if previous_titles == current_titles:\n",
    "            break\n",
    "\n",
    "        for j in range(len(Names)):\n",
    "            all_Name.append(Names[j].contents[0])\n",
    "\n",
    "        for z in range(len(links)):\n",
    "            all_links.append(links[z]['href'])\n",
    "\n",
    "        previous_titles = current_titles\n",
    "        page += 1\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기자, 날짜, 내용 (성공)\n",
    "import requests as rq\n",
    "from bs4 import BeautifulSoup\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "all_Reporter = []\n",
    "all_Date = []\n",
    "all_content = []\n",
    "\n",
    "def fetch_article_data(link):\n",
    "    try:\n",
    "        res_2 = rq.get(link, headers={\n",
    "            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36'\n",
    "        })\n",
    "        soup_2 = BeautifulSoup(res_2.text, 'html.parser')\n",
    "        Reporter_2 = soup_2.select('div.info_view > .txt_info')\n",
    "        Dates_2 = soup_2.select('div.info_view > span > .num_date')\n",
    "        contents_2 = soup_2.select('#mArticle > div.news_view.fs_type1')\n",
    "        Reporter_text = Reporter_2[0].text\n",
    "        Dates_text = Dates_2[0].text\n",
    "        contents_3 = re.sub(r'\\s+', ' ', contents_2[0].text.replace(\"\\n\", \"\").strip())\n",
    "        return (Reporter_text, Dates_text, contents_3)\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    results = list(tqdm(executor.map(fetch_article_data, all_links), total=len(all_links)))\n",
    "\n",
    "for result in results:\n",
    "    all_Reporter.append(result[0])\n",
    "    all_Date.append(result[1])\n",
    "    all_content.append(result[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'pub_name' : all_Name,\n",
    "    'title' : all_title,\n",
    "    'author' : all_Reporter,\n",
    "    'publication_date' : all_Date,\n",
    "    'content' : all_content,\n",
    "    'article_url': all_links\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스티커 드디어 완성 (성공)\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "all_Sticker = []\n",
    "\n",
    "def process_link(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(link)\n",
    "    try:\n",
    "        sticker_element = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"alex_action_emotion\"]'))\n",
    "        )\n",
    "        recommend_text = re.sub(r\"(\\d)([가-힣])\", r\"\\1 \\2\", sticker_element.text.replace(\"\\n\", \"\"))\n",
    "        return recommend_text\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = [executor.submit(process_link, link) for link in all_links]\n",
    "    \n",
    "    for future in tqdm(futures, desc=\"Processing\"):\n",
    "        try:\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                all_Sticker.append(result)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.0.3-cp38-cp38-win_amd64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\compro\\.conda\\envs\\nlp\\lib\\site-packages (from pandas) (2.9.0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting numpy>=1.20.3 (from pandas)\n",
      "  Using cached numpy-1.24.4-cp38-cp38-win_amd64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\compro\\.conda\\envs\\nlp\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pandas-2.0.3-cp38-cp38-win_amd64.whl (10.8 MB)\n",
      "Using cached numpy-1.24.4-cp38-cp38-win_amd64.whl (14.9 MB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-1.24.4 pandas-2.0.3 pytz-2024.1 tzdata-2024.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install requests\n",
    "#%pip install beautifulsoup4\n",
    "#%pip install tqdm\n",
    "#%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
